## Второй этап "Сравнение"
После полученяи прогноза, начинается процес сравнения прогноза 
с результатом, этот этап показывает на сколько проихошел промаш
прогноза, для вычисленеия используется метод 
**Среднеквадратичную ошибки**

### Среднеквадратичная ошибка
Ошибка всегда должна быть положительной, буть то промах на 2
сантиметра в верх или низ.

Есть вес и есть значение, их перемножение дает нам прогноз, из 
котррого мы вычитаем `goal_pred` о котором позже, далее
мы вычитаем из прогноза `goal_pred` и получившиеся число 
умножаем само на себя, в результате получаем чистую ошибку:

goal_pred - это получившийся результат реального мира.

    know_weight = 0.5 
    input = 0.5
    goal_pred = 0.8 

    pred = input * know_weight
    error = (pred - goal_pred)**2

    print(error)

Умножение ошибки саму на себя требуется чтобы получить положительное
число, но возведение в квадрат увеличивает ошибку если она больше 1 и 
уменьшает ее если она меньше 1, да это так и есть большие ошибки 
преувеличиваются а мелкие уменьшаются, на этом этапе это нормально,
на мелкие ошибки пока не обращаем внимание, берем только большие.

Это и есть метод Среднеквадратичной ошибки, и наша цель это сведение
этой средней ошибки к нулю.

По сути все обучение сводится к изменению `know_weight` для того чтобы
получившийся реультат был сведен к нулю, но в какую сторону менять 
значение в верх или низ ?

### Метод холодно/горячо
Простейший способ это изменять значение know_weight то в верх то в низ,
что позволяет увидеть когда ошибка увеличивается а когда уменьшается, и
найдя сторону в которую надо изменять значение, просто изменяем 
вес know_weight до такой степени чтосводит ошибку к нулю.

    def neural_network(input, weight):
        prediction = input * weight
        return prediction

    number_of_toes = [8.5]
    win_or_lose_binary = [1]

    input = number_of_toes[0]
    true = win_or_lose_binary[0]

    weight = 0.1 # вес
    lr = 0.01 # значение на которое изменяем вес

    # pred = neural_network(input, weight)
    pred = neural_network(input, weight)
    error = (pred - true) ** 2

    print("Чистая ошибка = ", error)

Выполнив программу получим что ошибка равна 0.022 далее, для 
уменьшения ошибки заменим строчку:

    pred = neural_network(input, weight + lr)

Таким образом мы изменяем наш вес на некотрое значение и видим что
чистая ошибка была уменьшена, и теперь составляет 0.004

Это привело к уменьшению ошибки, значит мы давим в правильном 
направлении.

Финальная версия расчета весов методом холодно/горячо:

    weight = 0.5
    input = 0.5
    goal_prediction = 0.8

    # Шаг изменения веса в каждой итерации
    step_amount = 0.001

    for iteration in range(1101):
        
        # Получаем прогноз и его ошибку
        prediction = input * weight
        error = (prediction - goal_prediction) ** 2

        print("Error:" + str(error) + " Prediction:" + str(prediction))

        # Расчет прогноза с повышением веса
        up_prediction = input * (weight + step_amount)
        up_error = (goal_prediction - up_prediction) ** 2

        # Расчет прогноза с понижением веса
        down_prediction = input * (weight - step_amount)
        down_error = (goal_prediction - down_prediction) ** 2

        # Если понижение веса дало меньшую ошибку чем его подьем
        # то понимжаем вес еще больше, и наоборот.
        if (down_error < up_error):
            weight = weight - step_amount
        elif (down_error > up_error):
            weight = weight + step_amount

В результате видим как прогноз увеличивается от 0.25 до
желаемых 0.8 в то время как ошибка уменьшается:

    Error: 0.30250000000000005 Prediction:0.25
    Error: 0.05904900000000297 Prediction:0.5569999999999939
    Error: 1.0799505792475652e-27 Prediction:0.7999999999999672

Прогноз растет а ошибка уменьшается.

### Особенности метода холодно/горячо
Этот метод очень прост, мы вичисляем прогноз с повышенным весом и 
пониженным, сверяем в каком из них ошибка меньше и продолжаем идти 
в этом напрвылении, многократное повторени этой процедуры приведет 
к уменьшению ошибки до нуля.

1) Первая проблема данного метода он неэффективен.
2) Иногда невозможно добиться идеальной точности прогнозирования.
    step_amount выбирается произвольно, что для большой и для малой 
   ошибки, и из за произвольности значения step_amount мы не можем
   узнать правильно значение веса.

Тоесть данный способ подходит для вычисления направления в которое 
следует изменять вес, но ничего не говорит о величине на которую
надо изменять.

### Градиентный спуск
### Вычисление напрвления и величины из ошибки
Измерим ошибку и определим направление и величину изменений, если 
раньше для этого мы использовали обычные условия, изменяя вес в разны 
стороны и смотря где ошибка будет меньше,о теперь мы используем 
другой метод.

Градиентный спуск - это способ который позволяет сразу понять направление
тоесть (+ или -) куда стоит давить вес, и позволяет вычислить величину на
которую следует изменить вес:

У нас ест вес, входное значение и результат.
 
    weight = 0.5
    goal_pred = 0.8
    input = 0.5

С начала вычисляем прогноз, умножая входное значение на веса.

    pred = input * weight

Теперь у нас есть прогноз и результат, далее вычисляем чистую 
ошибку, вычитаем из прогноза реальный результат.

    error = (pred - goal_pred)

Так мы получили чистую ошибку, не квадратичную когда умножаем ее на
саму семя, а только чистую, а после умножаем полученную чистую 
ошибку на входное значение:

    direction_and_amount = (pred - goal_pred) * input

Таким образом получаем значение которое можно сразу использовать 
для изменения веса.

    weight = weight - direction_and_amount

Самое интересное тут это то что градиентный спуск `direction_and_amount` 
моэеть дать как положительно так и отрицательное число, так что когда
мы хотим изменить веса с его помощью, мы вычитаем этот градинт из весов
либо он будет положительный либо отрицательный и тогда поменяет свой 
знак.

Пример работы программы с применением градиентного спуска:

    вес = 0.5 
    результат = 0.8 
    вход = 0.5

    // Первый проход
    Прогноз: 0.5 * 0.5 = 0.25
    Квадратичный метод :  0.30250000000000005
    Градиентный спуск: (0.25 - 0.8) * 0.5 = -0.275
    Изменяем веса:  0.5 - -0.275 = 0.775
    Error:0.30250000000000005 Prediction:0.25

    // Второй
    Прогноз: 0.5 * 0.775 = 0.3875
    Квадратичный метод :  0.17015625000000004
    Градиентный спуск: (0.3875 - 0.8) * 0.5 = -0.20625000000000002
    Изменяем веса:  0.775 - -0.20625000000000002 = 0.9812500000000001
    Error:0.17015625000000004 Prediction:0.3875

    // Двадцатый
    Прогноз: 0.5 * 1.5953488891562302 = 0.7976744445781151
    Квадратичный метод :  5.408208020258491e-06
    Градиентный спуск: (0.7976744445781151 - 0.8) * 0.5 = -0.0011627777109424753
    Изменяем веса:  1.5953488891562302 - -0.0011627777109424753 = 1.5965116668671726
    Error:5.408208020258491e-06 Prediction:0.7976744445781151

Если мы просто изменяли вес на какоето малое значение как делали это раньше 
то нам требовалось делать это вплоть до 1101 шагов, тут же все происходит 
за 20 шаговв. 

### Чувствительность
Чувствительность - одно из названий направления и величины изменения веса.

Главное что требуется понять для работы это насколько ошибка чувствительна 
к изменению веса, тоесть насколько изменение одной переменной веса повлечет
за собой изменение переменной ошибки.

Связь между ошибкой и весом точно можно выявить при помощи вычисления
среднеквадратичной, как мы уже видили это раньше:

    error = ((input * weight) - goal_pred) ** 2

По сути эту чувствительность можно описать словом - производная, тоесть 
на сколько измениться x при изменении Y

Если производная положительная то и изенение будет в томже направлении
и тут же описывается и величина этого изменения:

    one = two * 2

    one = 2 
    two = 4

    (2-1) = (2-1) * 2
    1 = 2

    (2) = (2)*2
    2 = 4

    (2+1) = (2+1)*2
    3 = 6

На этом примере можно увидеть что при помощи производной мы можем измерить 
в какую сторону и на сколько изменить одно переменная при изменении другой.
   
### Производная
Вычисление средннеквадратичной ошибки:

    error = ((input * weight) - goal_pred) ** 2

Поскольку input и goal_pred фиксированные то перепишем в след вид:

    error = ((0.5 * weight) - 0.8) ** 2

То есть для изменения нам доступно только 2 переменных.

Нейр сеть - это всего лишь набор весовых коэффициентов, что используются для
вычисления функции ошибки, и для любой функции ошибки какой бы она ни была, 
можно вычислить отношение между весом и ошибкой.





























